{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_filename_column(df):\n",
    "    if 'filename' in df.keys():\n",
    "        df.rename(columns={'filename': 'Filename'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_txt(txt):\n",
    "    if type(txt) == type(2):\n",
    "        return txt\n",
    "    else:\n",
    "        return(txt.replace('.txt', ''))\n",
    "\n",
    "def combine_psuedos_with_scores(df, scores_df):\n",
    "    df['Filename'] = df['Filename'].apply(drop_txt).astype('int')\n",
    "    scores_df['Psuedos'] = scores_df['Psuedos'].astype('int')\n",
    "\n",
    "    combined_df = df.merge(scores_df, how='right', left_on='Filename', right_on='Psuedos')\n",
    "    combined_df = combined_df.dropna()\n",
    "    return combined_df\n",
    "\n",
    "def categorize_grades(score):\n",
    "    if score >= 0.9:\n",
    "        return 4\n",
    "    elif score >= 0.8:\n",
    "        return 3\n",
    "    elif score >= 0.7:\n",
    "        return 2\n",
    "    elif score >= 0.6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extract_all_features(df):\n",
    "    list_of_col_names = df.keys().tolist()\n",
    "    if 'Filename' in list_of_col_names:\n",
    "        list_of_col_names.remove('Filename')\n",
    "    elif 'filename' in list_of_col_names:\n",
    "        list_of_col_names.remove('filename')\n",
    "    return list_of_col_names\n",
    "\n",
    "def create_X_y(df, scores_df):\n",
    "    combined_df = combine_psuedos_with_scores(df, scores_df)\n",
    "\n",
    "    response_var = combined_df['ChatGPT Percent Score']\n",
    "    combined_df.drop(['Filename', 'Psuedos', 'ChatGPT Percent Score', 'Response Word Count'], axis=1, inplace=True)\n",
    "    predictor_vars = combined_df\n",
    "\n",
    "    return {'X': predictor_vars, \n",
    "            'y': response_var\n",
    "    }\n",
    "\n",
    "def split_and_fit(temp_df, scores_df, pred_name):\n",
    "    temp_X_y_dict = create_X_y(temp_df, scores_df)\n",
    "\n",
    "    scaled_X = temp_X_y_dict['X'].copy()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = pd.DataFrame(scaler.fit_transform(scaled_X), columns=scaled_X.columns)\n",
    "\n",
    "    y = temp_X_y_dict['y'].apply(categorize_grades)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2)\n",
    "\n",
    "    temp_df_dict = {\n",
    "        'name': pred_name, \n",
    "        'df': temp_df, \n",
    "        'X_train': X_train, \n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    return temp_df_dict\n",
    "\n",
    "def create_combined_features_df(base_dir, combined_dict):\n",
    "    list_of_partial_dfs = []\n",
    "\n",
    "    for pred_name in ['taaco', 'taaled', 'taales', 'taassc']:\n",
    "        temp_pred_results_file_path = os.path.join(base_dir, 'predictor_results', f'{pred_name}_results.csv')\n",
    "        temp_df = reconcile_filename_column(pd.read_csv(temp_pred_results_file_path))\n",
    "        try:\n",
    "            reduced_df = temp_df[combined_dict[pred_name] + ['Filename']]\n",
    "        except: \n",
    "            reduced_df = temp_df[combined_dict[pred_name] + ['filename']]\n",
    "        list_of_partial_dfs.append(reduced_df)\n",
    "\n",
    "    stable_features_df = pd.concat(list_of_partial_dfs, axis=1)\n",
    "    return stable_features_df\n",
    "\n",
    "def load_dfs(base_dir, math=0):\n",
    "    '''\n",
    "    Takes base features and target, then combines with some linguistic measures. \n",
    "    Scales features using StandardScaler()\n",
    "    '''\n",
    "    combined_scores_file_path = os.path.join(base_dir, 'data', 'combined_responses_scores_added.xlsx')\n",
    "    scores_df = pd.read_excel(combined_scores_file_path, index_col=0)\n",
    "    scores_df = scores_df[['Psuedos', 'ChatGPT Percent Score', 'Response Word Count', 'Course']]\n",
    "    if math == 0:\n",
    "        scores_df = scores_df[scores_df['Course'] != 'MATH 111']\n",
    "    \n",
    "    scores_df = scores_df.drop(['Course'], axis=1)\n",
    "\n",
    "    list_of_df_dicts = []\n",
    "    combined_dict = {}\n",
    "\n",
    "    for pred_name in ['taaco', 'taaled', 'taales', 'taassc']:\n",
    "        temp_pred_results_file_path = os.path.join(base_dir, 'predictor_results', f'{pred_name}_results.csv')\n",
    "        temp_df = reconcile_filename_column(pd.read_csv(temp_pred_results_file_path))\n",
    "        list_of_df_dicts.append(split_and_fit(temp_df, scores_df, pred_name))\n",
    "        combined_dict[pred_name] = extract_all_features(temp_df)\n",
    "\n",
    "    selected_features_df = create_combined_features_df(base_dir, combined_dict)\n",
    "    selected_features_df = selected_features_df.loc[:,~selected_features_df.columns.duplicated()].copy() # Remove multiple copies of \"Filename\" column\n",
    "    selected_features_df_dict = split_and_fit(selected_features_df, scores_df, 'all_features')\n",
    "    list_of_df_dicts.append(selected_features_df_dict)\n",
    "\n",
    "    return [list_of_df_dicts, scores_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=os.getcwd()\n",
    "list_of_df_dicts, scores_df = load_dfs(base_dir, math=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_distribution(scores_df):\n",
    "    df = scores_df.copy()\n",
    "    df['letter_grades'] = df['ChatGPT Percent Score'].apply(categorize_grades)\n",
    "    return df['letter_grades']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_distribution(scores_df).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_best_model(df_dict):\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 4, 6, 8],\n",
    "        'class_weight': ['balanced'],\n",
    "        'n_estimators': [10, 20, 30, 40, 50],\n",
    "        #'n_estimators': [10]\n",
    "    }\n",
    "    search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    search.fit(df_dict['X_train'], df_dict['y_train'])\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "def run_rf(df_dict, top_n):\n",
    "    clf = grid_search_best_model(df_dict)\n",
    "    df_dict['accuracy'] = clf.score(df_dict['X_test'], df_dict['y_test'])\n",
    "    df_dict['feature_importances'] = clf.feature_importances_\n",
    "    top_features_series = pd.Series(clf.feature_importances_, index=df_dict['X_train'].columns).nlargest(n=top_n)\n",
    "    df_dict[f'top_{top_n}_feature_names'] = top_features_series.index\n",
    "    df_dict[f'top_{top_n}_feature_importances'] = top_features_series.tolist()\n",
    "    return df_dict\n",
    "\n",
    "def graph_feature_importance(df_dict):\n",
    "    plt.barh(df_dict['X_train'].columns, df_dict['feature_importances'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance in Random Forest Classifier')\n",
    "    plt.show()\n",
    "\n",
    "def run_k_models(df_dict, top_n, k_models):\n",
    "    list_of_run_df_dicts = []\n",
    "\n",
    "    for run_index in range(0, k_models): \n",
    "        temp_df_dict = df_dict.copy()\n",
    "        temp_df_dict['run_index'] = run_index\n",
    "        temp_df_dict = run_rf(temp_df_dict, top_n)\n",
    "        list_of_run_df_dicts.append(temp_df_dict)\n",
    "    return list_of_run_df_dicts\n",
    "\n",
    "def check_accuracy_df(list_of_run_df_dicts):\n",
    "    list_of_acc = []\n",
    "    for run_index in range(0, len(list_of_run_df_dicts)): \n",
    "        temp_df_dict = list_of_run_df_dicts[run_index]\n",
    "        temp_acc_dict = {}\n",
    "        temp_acc_dict[temp_df_dict['name']] = temp_df_dict['accuracy']\n",
    "        list_of_acc.append(temp_acc_dict)\n",
    "    return pd.DataFrame(list_of_acc)\n",
    "\n",
    "def check_feature_stability(list_of_run_df_dicts, top_n):\n",
    "    stability_df = pd.DataFrame(list_of_run_df_dicts)\n",
    "    check_stability_df = stability_df.copy()\n",
    "    check_stability_df = check_stability_df.explode(f'top_{top_n}_feature_names')\n",
    "    check_counts = check_stability_df[f'top_{top_n}_feature_names'].value_counts(normalize=True)\n",
    "    check_counts = check_counts * top_n # fix normalization after exploding\n",
    "    check_counts = check_counts[check_counts > 0.3]\n",
    "    return check_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(list_of_df_dicts):\n",
    "    list_of_all_models = []\n",
    "    for temp_dict in list_of_df_dicts:\n",
    "        list_of_temp_run_df_dicts = run_k_models(temp_dict, 3, 1000)\n",
    "        list_of_all_models.append(list_of_temp_run_df_dicts)\n",
    "    return list_of_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_models = run_all_models(list_of_df_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def report_accuracy(list_of_all_models):\n",
    "#    for temp_all_runs in list_of_all_models:\n",
    "\n",
    "\n",
    "def report_feature_stability(list_of_all_models):\n",
    "    for temp_all_runs in list_of_all_models:\n",
    "        temp_counts = check_feature_stability(temp_all_runs, 3)\n",
    "        print(temp_counts)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_feature_stability(list_of_all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
